{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, GlobalMaxPool1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "data = pd.read_excel('data.xlsx') # Substitua pelo nome do arquivo\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 57ms/step - accuracy: 0.5708 - loss: 1.3392 - val_accuracy: 0.9270 - val_loss: 0.2865\n",
      "Epoch 2/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 59ms/step - accuracy: 0.9418 - loss: 0.2377 - val_accuracy: 0.9499 - val_loss: 0.1880\n",
      "Epoch 3/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 58ms/step - accuracy: 0.9653 - loss: 0.1440 - val_accuracy: 0.9561 - val_loss: 0.1631\n",
      "Epoch 4/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 57ms/step - accuracy: 0.9770 - loss: 0.0914 - val_accuracy: 0.9561 - val_loss: 0.1653\n",
      "Epoch 5/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 58ms/step - accuracy: 0.9805 - loss: 0.0768 - val_accuracy: 0.9585 - val_loss: 0.1692\n",
      "Epoch 6/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 61ms/step - accuracy: 0.9834 - loss: 0.0699 - val_accuracy: 0.9599 - val_loss: 0.1671\n",
      "Epoch 7/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 60ms/step - accuracy: 0.9851 - loss: 0.0555 - val_accuracy: 0.9532 - val_loss: 0.1937\n",
      "Epoch 8/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 61ms/step - accuracy: 0.9894 - loss: 0.0501 - val_accuracy: 0.9570 - val_loss: 0.1991\n",
      "Epoch 9/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 64ms/step - accuracy: 0.9868 - loss: 0.0449 - val_accuracy: 0.9499 - val_loss: 0.2056\n",
      "Epoch 10/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - accuracy: 0.9878 - loss: 0.0396 - val_accuracy: 0.9570 - val_loss: 0.1872\n",
      "Epoch 11/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 53ms/step - accuracy: 0.9910 - loss: 0.0354 - val_accuracy: 0.9594 - val_loss: 0.1972\n",
      "Epoch 12/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 56ms/step - accuracy: 0.9924 - loss: 0.0287 - val_accuracy: 0.9551 - val_loss: 0.2260\n",
      "Epoch 13/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 52ms/step - accuracy: 0.9912 - loss: 0.0329 - val_accuracy: 0.9556 - val_loss: 0.2160\n",
      "Epoch 14/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - accuracy: 0.9925 - loss: 0.0261 - val_accuracy: 0.9575 - val_loss: 0.2254\n",
      "Epoch 15/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - accuracy: 0.9931 - loss: 0.0238 - val_accuracy: 0.9585 - val_loss: 0.2216\n",
      "Epoch 16/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 65ms/step - accuracy: 0.9951 - loss: 0.0178 - val_accuracy: 0.9527 - val_loss: 0.2272\n",
      "Epoch 17/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 69ms/step - accuracy: 0.9956 - loss: 0.0141 - val_accuracy: 0.9532 - val_loss: 0.2274\n",
      "Epoch 18/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 68ms/step - accuracy: 0.9922 - loss: 0.0290 - val_accuracy: 0.9537 - val_loss: 0.2296\n",
      "Epoch 19/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 69ms/step - accuracy: 0.9932 - loss: 0.0218 - val_accuracy: 0.9551 - val_loss: 0.2393\n",
      "Epoch 20/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 61ms/step - accuracy: 0.9964 - loss: 0.0135 - val_accuracy: 0.9551 - val_loss: 0.2507\n",
      "Epoch 21/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 62ms/step - accuracy: 0.9953 - loss: 0.0204 - val_accuracy: 0.9594 - val_loss: 0.2435\n",
      "Epoch 22/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 59ms/step - accuracy: 0.9959 - loss: 0.0153 - val_accuracy: 0.9589 - val_loss: 0.2324\n",
      "Epoch 23/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 72ms/step - accuracy: 0.9954 - loss: 0.0116 - val_accuracy: 0.9570 - val_loss: 0.2376\n",
      "Epoch 24/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 68ms/step - accuracy: 0.9963 - loss: 0.0098 - val_accuracy: 0.9556 - val_loss: 0.2652\n",
      "Epoch 25/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - accuracy: 0.9961 - loss: 0.0117 - val_accuracy: 0.9527 - val_loss: 0.2844\n",
      "Epoch 26/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 62ms/step - accuracy: 0.9979 - loss: 0.0068 - val_accuracy: 0.9585 - val_loss: 0.2748\n",
      "Epoch 27/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 66ms/step - accuracy: 0.9964 - loss: 0.0098 - val_accuracy: 0.9551 - val_loss: 0.2894\n",
      "Epoch 28/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 65ms/step - accuracy: 0.9960 - loss: 0.0143 - val_accuracy: 0.9570 - val_loss: 0.2622\n",
      "Epoch 29/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 69ms/step - accuracy: 0.9928 - loss: 0.0149 - val_accuracy: 0.9575 - val_loss: 0.2691\n",
      "Epoch 30/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 67ms/step - accuracy: 0.9967 - loss: 0.0132 - val_accuracy: 0.9527 - val_loss: 0.3064\n",
      "Epoch 31/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 67ms/step - accuracy: 0.9959 - loss: 0.0106 - val_accuracy: 0.9542 - val_loss: 0.2710\n",
      "Epoch 32/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 62ms/step - accuracy: 0.9929 - loss: 0.0236 - val_accuracy: 0.9551 - val_loss: 0.2493\n",
      "Epoch 33/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 79ms/step - accuracy: 0.9961 - loss: 0.0128 - val_accuracy: 0.9537 - val_loss: 0.2704\n",
      "Epoch 34/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 63ms/step - accuracy: 0.9970 - loss: 0.0081 - val_accuracy: 0.9556 - val_loss: 0.2858\n",
      "Epoch 35/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 62ms/step - accuracy: 0.9970 - loss: 0.0080 - val_accuracy: 0.9527 - val_loss: 0.3142\n",
      "Epoch 36/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 67ms/step - accuracy: 0.9972 - loss: 0.0069 - val_accuracy: 0.9561 - val_loss: 0.3096\n",
      "Epoch 37/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 62ms/step - accuracy: 0.9974 - loss: 0.0087 - val_accuracy: 0.9547 - val_loss: 0.3331\n",
      "Epoch 38/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 62ms/step - accuracy: 0.9966 - loss: 0.0140 - val_accuracy: 0.9561 - val_loss: 0.3254\n",
      "Epoch 39/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 65ms/step - accuracy: 0.9968 - loss: 0.0132 - val_accuracy: 0.9609 - val_loss: 0.2742\n",
      "Epoch 40/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 63ms/step - accuracy: 0.9957 - loss: 0.0104 - val_accuracy: 0.9566 - val_loss: 0.2889\n",
      "Epoch 41/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 78ms/step - accuracy: 0.9955 - loss: 0.0138 - val_accuracy: 0.9585 - val_loss: 0.2949\n",
      "Epoch 42/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 82ms/step - accuracy: 0.9978 - loss: 0.0077 - val_accuracy: 0.9547 - val_loss: 0.3217\n",
      "Epoch 43/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 81ms/step - accuracy: 0.9977 - loss: 0.0061 - val_accuracy: 0.9551 - val_loss: 0.3467\n",
      "Epoch 44/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 90ms/step - accuracy: 0.9976 - loss: 0.0062 - val_accuracy: 0.9589 - val_loss: 0.3125\n",
      "Epoch 45/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 92ms/step - accuracy: 0.9973 - loss: 0.0052 - val_accuracy: 0.9580 - val_loss: 0.3121\n",
      "Epoch 46/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 91ms/step - accuracy: 0.9974 - loss: 0.0057 - val_accuracy: 0.9589 - val_loss: 0.3481\n",
      "Epoch 47/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 84ms/step - accuracy: 0.9975 - loss: 0.0062 - val_accuracy: 0.9566 - val_loss: 0.3661\n",
      "Epoch 48/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 78ms/step - accuracy: 0.9977 - loss: 0.0061 - val_accuracy: 0.9561 - val_loss: 0.3370\n",
      "Epoch 49/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 77ms/step - accuracy: 0.9965 - loss: 0.0153 - val_accuracy: 0.9561 - val_loss: 0.3017\n",
      "Epoch 50/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 80ms/step - accuracy: 0.9963 - loss: 0.0111 - val_accuracy: 0.9575 - val_loss: 0.2920\n"
     ]
    }
   ],
   "source": [
    "# 1. Carregando os dados\n",
    "\n",
    "X = data[\"X_Text_input\"]\n",
    "y = data[\"Y_SISTEMA_output\"]  # Trabalhando apenas com Y_SISTEMA_output\n",
    "\n",
    "\n",
    "# 2. Pré-processando a saída\n",
    "# Convertendo as categorias da saída para valores numéricos\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# 3. Dividindo os dados\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4. Tokenização e padronização das sequências\n",
    "max_words = 10000  # Número máximo de palavras no vocabulário\n",
    "max_len = 100      # Tamanho máximo de sequência\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<UNK>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "# 5. Criando o modelo\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=max_words, output_dim=128),\n",
    "    LSTM(128, return_sequences=True),\n",
    "    GlobalMaxPool1D(),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')  # Classificação multiclasse\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# 6. Treinando o modelo\n",
    "history = model.fit(\n",
    "    X_train_padded, y_train,\n",
    "    validation_data=(X_test_padded, y_test),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - accuracy: 0.9555 - loss: 0.2811\n",
      "Loss: 0.2920001447200775, Accuracy: 0.9575179219245911\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step\n",
      "['FRENAGEM' 'CCT' 'RODEIRO' 'CCT' 'ENTRESSAFRA']\n"
     ]
    }
   ],
   "source": [
    "# 7. Avaliando o modelo\n",
    "loss, accuracy = model.evaluate(X_test_padded, y_test)\n",
    "print(f\"Loss: {loss}, Accuracy: {accuracy}\")\n",
    "\n",
    "# 8. Fazendo previsões\n",
    "y_pred = model.predict(X_test_padded)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Decodificando os rótulos previstos\n",
    "pred_labels = label_encoder.inverse_transform(y_pred_classes)\n",
    "print(pred_labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "#salvando movel\n",
    "from tensorflow.keras.models import save_model\n",
    "import pickle\n",
    "\n",
    "# Salvar o modelo treinado\n",
    "model.save(\"modelo_classificacao_y_Sistema.h5\")\n",
    "\n",
    "# Salvar o tokenizer\n",
    "with open(\"tokenizer_Sistema.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "# Salvar o LabelEncoder\n",
    "with open(\"label_encoder_Sistema.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label_encoder, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 303ms/step\n",
      "Entrada: 02/01/2024-|CA||4E||RODA BANDAGEM ESCOAMENTO\n",
      "Previsão: RODEIRO\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "\n",
    "# Carregar o modelo\n",
    "modelo_carregado = load_model(\"modelo_classificacao_y_Sistema.h5\")\n",
    "\n",
    "# Carregar o tokenizer\n",
    "with open(\"tokenizer_Sistema.pkl\", \"rb\") as f:\n",
    "    tokenizer_carregado = pickle.load(f)\n",
    "\n",
    "# Carregar o LabelEncoder\n",
    "with open(\"label_encoder_Sistema.pkl\", \"rb\") as f:\n",
    "    label_encoder_carregado = pickle.load(f)\n",
    "\n",
    "# Fazer previsão com o modelo carregado\n",
    "novo_texto = [\"02/01/2024-|CA||4E||RODA BANDAGEM ESCOAMENTO\"]\n",
    "novo_texto_seq = tokenizer_carregado.texts_to_sequences(novo_texto)\n",
    "novo_texto_padded = pad_sequences(novo_texto_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "pred_proba = modelo_carregado.predict(novo_texto_padded)\n",
    "pred_classe = np.argmax(pred_proba, axis=1)\n",
    "pred_label = label_encoder_carregado.inverse_transform(pred_classe)\n",
    "\n",
    "print(f\"Entrada: {novo_texto[0]}\")\n",
    "print(f\"Previsão: {pred_label[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 80ms/step - accuracy: 0.5368 - loss: 2.0098 - val_accuracy: 0.8368 - val_loss: 0.6792\n",
      "Epoch 2/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 89ms/step - accuracy: 0.8571 - loss: 0.6315 - val_accuracy: 0.8874 - val_loss: 0.4677\n",
      "Epoch 3/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 84ms/step - accuracy: 0.8947 - loss: 0.4498 - val_accuracy: 0.9088 - val_loss: 0.4008\n",
      "Epoch 4/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 87ms/step - accuracy: 0.9093 - loss: 0.3729 - val_accuracy: 0.9103 - val_loss: 0.3651\n",
      "Epoch 5/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 82ms/step - accuracy: 0.9188 - loss: 0.3186 - val_accuracy: 0.9212 - val_loss: 0.3306\n",
      "Epoch 6/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 78ms/step - accuracy: 0.9387 - loss: 0.2443 - val_accuracy: 0.9227 - val_loss: 0.3155\n",
      "Epoch 7/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 86ms/step - accuracy: 0.9426 - loss: 0.2210 - val_accuracy: 0.9232 - val_loss: 0.3205\n",
      "Epoch 8/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 78ms/step - accuracy: 0.9485 - loss: 0.2019 - val_accuracy: 0.9298 - val_loss: 0.3138\n",
      "Epoch 9/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 84ms/step - accuracy: 0.9601 - loss: 0.1586 - val_accuracy: 0.9270 - val_loss: 0.2997\n",
      "Epoch 10/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 75ms/step - accuracy: 0.9549 - loss: 0.1640 - val_accuracy: 0.9212 - val_loss: 0.3359\n",
      "Epoch 11/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 78ms/step - accuracy: 0.9605 - loss: 0.1403 - val_accuracy: 0.9274 - val_loss: 0.3439\n",
      "Epoch 12/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 74ms/step - accuracy: 0.9627 - loss: 0.1364 - val_accuracy: 0.9274 - val_loss: 0.3398\n",
      "Epoch 13/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 91ms/step - accuracy: 0.9695 - loss: 0.1025 - val_accuracy: 0.9279 - val_loss: 0.3400\n",
      "Epoch 14/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 88ms/step - accuracy: 0.9674 - loss: 0.1177 - val_accuracy: 0.9322 - val_loss: 0.3795\n",
      "Epoch 15/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 81ms/step - accuracy: 0.9705 - loss: 0.0951 - val_accuracy: 0.9351 - val_loss: 0.3533\n",
      "Epoch 16/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 87ms/step - accuracy: 0.9719 - loss: 0.1006 - val_accuracy: 0.9313 - val_loss: 0.4094\n",
      "Epoch 17/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 79ms/step - accuracy: 0.9702 - loss: 0.0912 - val_accuracy: 0.9260 - val_loss: 0.3937\n",
      "Epoch 18/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 80ms/step - accuracy: 0.9771 - loss: 0.0778 - val_accuracy: 0.9260 - val_loss: 0.3865\n",
      "Epoch 19/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 86ms/step - accuracy: 0.9788 - loss: 0.0716 - val_accuracy: 0.9294 - val_loss: 0.4058\n",
      "Epoch 20/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 71ms/step - accuracy: 0.9811 - loss: 0.0603 - val_accuracy: 0.9279 - val_loss: 0.4338\n",
      "Epoch 21/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 86ms/step - accuracy: 0.9820 - loss: 0.0624 - val_accuracy: 0.9294 - val_loss: 0.4399\n",
      "Epoch 22/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 85ms/step - accuracy: 0.9821 - loss: 0.0632 - val_accuracy: 0.9260 - val_loss: 0.4458\n",
      "Epoch 23/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 85ms/step - accuracy: 0.9837 - loss: 0.0549 - val_accuracy: 0.9294 - val_loss: 0.4459\n",
      "Epoch 24/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 84ms/step - accuracy: 0.9839 - loss: 0.0557 - val_accuracy: 0.9327 - val_loss: 0.4739\n",
      "Epoch 25/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 76ms/step - accuracy: 0.9841 - loss: 0.0503 - val_accuracy: 0.9274 - val_loss: 0.4798\n",
      "Epoch 26/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 73ms/step - accuracy: 0.9852 - loss: 0.0414 - val_accuracy: 0.9255 - val_loss: 0.4732\n",
      "Epoch 27/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 80ms/step - accuracy: 0.9837 - loss: 0.0518 - val_accuracy: 0.9265 - val_loss: 0.4469\n",
      "Epoch 28/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 77ms/step - accuracy: 0.9898 - loss: 0.0313 - val_accuracy: 0.9270 - val_loss: 0.4676\n",
      "Epoch 29/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 83ms/step - accuracy: 0.9859 - loss: 0.0391 - val_accuracy: 0.9179 - val_loss: 0.5100\n",
      "Epoch 30/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 88ms/step - accuracy: 0.9900 - loss: 0.0353 - val_accuracy: 0.9265 - val_loss: 0.4978\n",
      "Epoch 31/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 83ms/step - accuracy: 0.9903 - loss: 0.0338 - val_accuracy: 0.9270 - val_loss: 0.5011\n",
      "Epoch 32/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 76ms/step - accuracy: 0.9884 - loss: 0.0403 - val_accuracy: 0.9232 - val_loss: 0.5503\n",
      "Epoch 33/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 77ms/step - accuracy: 0.9865 - loss: 0.0384 - val_accuracy: 0.9270 - val_loss: 0.5580\n",
      "Epoch 34/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 79ms/step - accuracy: 0.9881 - loss: 0.0334 - val_accuracy: 0.9294 - val_loss: 0.5456\n",
      "Epoch 35/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 84ms/step - accuracy: 0.9926 - loss: 0.0244 - val_accuracy: 0.9284 - val_loss: 0.5283\n",
      "Epoch 36/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 80ms/step - accuracy: 0.9894 - loss: 0.0310 - val_accuracy: 0.9212 - val_loss: 0.5768\n",
      "Epoch 37/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 75ms/step - accuracy: 0.9913 - loss: 0.0251 - val_accuracy: 0.9279 - val_loss: 0.5607\n",
      "Epoch 38/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 81ms/step - accuracy: 0.9921 - loss: 0.0260 - val_accuracy: 0.9203 - val_loss: 0.5636\n",
      "Epoch 39/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 76ms/step - accuracy: 0.9929 - loss: 0.0222 - val_accuracy: 0.9236 - val_loss: 0.5376\n",
      "Epoch 40/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 73ms/step - accuracy: 0.9925 - loss: 0.0218 - val_accuracy: 0.9246 - val_loss: 0.5748\n",
      "Epoch 41/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 79ms/step - accuracy: 0.9910 - loss: 0.0303 - val_accuracy: 0.9227 - val_loss: 0.5997\n",
      "Epoch 42/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 80ms/step - accuracy: 0.9937 - loss: 0.0201 - val_accuracy: 0.9232 - val_loss: 0.5713\n",
      "Epoch 43/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 84ms/step - accuracy: 0.9915 - loss: 0.0241 - val_accuracy: 0.9217 - val_loss: 0.5700\n",
      "Epoch 44/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 99ms/step - accuracy: 0.9939 - loss: 0.0199 - val_accuracy: 0.9203 - val_loss: 0.5794\n",
      "Epoch 45/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 83ms/step - accuracy: 0.9914 - loss: 0.0208 - val_accuracy: 0.9198 - val_loss: 0.6172\n",
      "Epoch 46/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 85ms/step - accuracy: 0.9893 - loss: 0.0386 - val_accuracy: 0.9212 - val_loss: 0.6020\n",
      "Epoch 47/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 84ms/step - accuracy: 0.9912 - loss: 0.0235 - val_accuracy: 0.9227 - val_loss: 0.5797\n",
      "Epoch 48/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 77ms/step - accuracy: 0.9927 - loss: 0.0216 - val_accuracy: 0.9251 - val_loss: 0.5652\n",
      "Epoch 49/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 77ms/step - accuracy: 0.9920 - loss: 0.0214 - val_accuracy: 0.9255 - val_loss: 0.5784\n",
      "Epoch 50/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 83ms/step - accuracy: 0.9920 - loss: 0.0215 - val_accuracy: 0.9294 - val_loss: 0.5950\n",
      "Epoch 51/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 90ms/step - accuracy: 0.9952 - loss: 0.0192 - val_accuracy: 0.9241 - val_loss: 0.6077\n",
      "Epoch 52/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 81ms/step - accuracy: 0.9940 - loss: 0.0155 - val_accuracy: 0.9212 - val_loss: 0.6412\n",
      "Epoch 53/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 75ms/step - accuracy: 0.9945 - loss: 0.0172 - val_accuracy: 0.9265 - val_loss: 0.5605\n",
      "Epoch 54/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 82ms/step - accuracy: 0.9926 - loss: 0.0188 - val_accuracy: 0.9251 - val_loss: 0.5954\n",
      "Epoch 55/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 81ms/step - accuracy: 0.9923 - loss: 0.0173 - val_accuracy: 0.9203 - val_loss: 0.5788\n",
      "Epoch 56/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 83ms/step - accuracy: 0.9949 - loss: 0.0164 - val_accuracy: 0.9279 - val_loss: 0.6356\n",
      "Epoch 57/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 89ms/step - accuracy: 0.9934 - loss: 0.0200 - val_accuracy: 0.9246 - val_loss: 0.6158\n",
      "Epoch 58/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 93ms/step - accuracy: 0.9928 - loss: 0.0255 - val_accuracy: 0.9274 - val_loss: 0.6156\n",
      "Epoch 59/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 80ms/step - accuracy: 0.9954 - loss: 0.0135 - val_accuracy: 0.9208 - val_loss: 0.6358\n",
      "Epoch 60/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 67ms/step - accuracy: 0.9956 - loss: 0.0116 - val_accuracy: 0.9265 - val_loss: 0.6370\n",
      "Epoch 61/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 67ms/step - accuracy: 0.9951 - loss: 0.0111 - val_accuracy: 0.9294 - val_loss: 0.6278\n",
      "Epoch 62/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 67ms/step - accuracy: 0.9948 - loss: 0.0164 - val_accuracy: 0.9284 - val_loss: 0.6516\n",
      "Epoch 63/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 66ms/step - accuracy: 0.9954 - loss: 0.0128 - val_accuracy: 0.9232 - val_loss: 0.6492\n",
      "Epoch 64/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 66ms/step - accuracy: 0.9927 - loss: 0.0205 - val_accuracy: 0.9212 - val_loss: 0.7093\n",
      "Epoch 65/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 69ms/step - accuracy: 0.9962 - loss: 0.0148 - val_accuracy: 0.9260 - val_loss: 0.6856\n",
      "Epoch 66/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 66ms/step - accuracy: 0.9952 - loss: 0.0172 - val_accuracy: 0.9236 - val_loss: 0.6654\n",
      "Epoch 67/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 66ms/step - accuracy: 0.9945 - loss: 0.0146 - val_accuracy: 0.9251 - val_loss: 0.6782\n",
      "Epoch 68/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 68ms/step - accuracy: 0.9947 - loss: 0.0123 - val_accuracy: 0.9251 - val_loss: 0.6841\n",
      "Epoch 69/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 71ms/step - accuracy: 0.9954 - loss: 0.0132 - val_accuracy: 0.9265 - val_loss: 0.6887\n",
      "Epoch 70/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 67ms/step - accuracy: 0.9949 - loss: 0.0160 - val_accuracy: 0.9270 - val_loss: 0.6593\n",
      "Epoch 71/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 66ms/step - accuracy: 0.9942 - loss: 0.0152 - val_accuracy: 0.9251 - val_loss: 0.7265\n",
      "Epoch 72/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 67ms/step - accuracy: 0.9922 - loss: 0.0250 - val_accuracy: 0.9265 - val_loss: 0.7161\n",
      "Epoch 73/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 66ms/step - accuracy: 0.9912 - loss: 0.0241 - val_accuracy: 0.9265 - val_loss: 0.6947\n",
      "Epoch 74/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 66ms/step - accuracy: 0.9947 - loss: 0.0180 - val_accuracy: 0.9270 - val_loss: 0.6936\n",
      "Epoch 75/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 67ms/step - accuracy: 0.9933 - loss: 0.0162 - val_accuracy: 0.9274 - val_loss: 0.6616\n",
      "Epoch 76/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 68ms/step - accuracy: 0.9931 - loss: 0.0263 - val_accuracy: 0.9241 - val_loss: 0.6617\n",
      "Epoch 77/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 68ms/step - accuracy: 0.9962 - loss: 0.0106 - val_accuracy: 0.9232 - val_loss: 0.6697\n",
      "Epoch 78/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 68ms/step - accuracy: 0.9958 - loss: 0.0098 - val_accuracy: 0.9193 - val_loss: 0.6686\n",
      "Epoch 79/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 65ms/step - accuracy: 0.9954 - loss: 0.0118 - val_accuracy: 0.9251 - val_loss: 0.6704\n",
      "Epoch 80/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 64ms/step - accuracy: 0.9945 - loss: 0.0135 - val_accuracy: 0.9246 - val_loss: 0.6944\n",
      "Epoch 81/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 64ms/step - accuracy: 0.9955 - loss: 0.0121 - val_accuracy: 0.9255 - val_loss: 0.7248\n",
      "Epoch 82/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 69ms/step - accuracy: 0.9941 - loss: 0.0154 - val_accuracy: 0.9246 - val_loss: 0.7171\n",
      "Epoch 83/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 65ms/step - accuracy: 0.9935 - loss: 0.0130 - val_accuracy: 0.9236 - val_loss: 0.7390\n",
      "Epoch 84/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 63ms/step - accuracy: 0.9947 - loss: 0.0174 - val_accuracy: 0.9236 - val_loss: 0.7033\n",
      "Epoch 85/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 66ms/step - accuracy: 0.9937 - loss: 0.0196 - val_accuracy: 0.9227 - val_loss: 0.6993\n",
      "Epoch 86/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 64ms/step - accuracy: 0.9953 - loss: 0.0129 - val_accuracy: 0.9236 - val_loss: 0.7604\n",
      "Epoch 87/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 65ms/step - accuracy: 0.9964 - loss: 0.0087 - val_accuracy: 0.9217 - val_loss: 0.7333\n",
      "Epoch 88/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 63ms/step - accuracy: 0.9962 - loss: 0.0086 - val_accuracy: 0.9222 - val_loss: 0.7468\n",
      "Epoch 89/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 65ms/step - accuracy: 0.9972 - loss: 0.0078 - val_accuracy: 0.9241 - val_loss: 0.8001\n",
      "Epoch 90/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 64ms/step - accuracy: 0.9961 - loss: 0.0090 - val_accuracy: 0.9203 - val_loss: 0.7801\n",
      "Epoch 91/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 64ms/step - accuracy: 0.9964 - loss: 0.0086 - val_accuracy: 0.9189 - val_loss: 0.7828\n",
      "Epoch 92/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 70ms/step - accuracy: 0.9948 - loss: 0.0171 - val_accuracy: 0.9251 - val_loss: 0.7816\n",
      "Epoch 93/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 69ms/step - accuracy: 0.9939 - loss: 0.0143 - val_accuracy: 0.9208 - val_loss: 0.7450\n",
      "Epoch 94/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 69ms/step - accuracy: 0.9953 - loss: 0.0139 - val_accuracy: 0.9217 - val_loss: 0.7544\n",
      "Epoch 95/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 56ms/step - accuracy: 0.9958 - loss: 0.0108 - val_accuracy: 0.9241 - val_loss: 0.7694\n",
      "Epoch 96/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 59ms/step - accuracy: 0.9958 - loss: 0.0105 - val_accuracy: 0.9251 - val_loss: 0.7754\n",
      "Epoch 97/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 59ms/step - accuracy: 0.9959 - loss: 0.0104 - val_accuracy: 0.9232 - val_loss: 0.7771\n",
      "Epoch 98/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 56ms/step - accuracy: 0.9950 - loss: 0.0150 - val_accuracy: 0.9265 - val_loss: 0.7780\n",
      "Epoch 99/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 56ms/step - accuracy: 0.9970 - loss: 0.0103 - val_accuracy: 0.9251 - val_loss: 0.7299\n",
      "Epoch 100/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 58ms/step - accuracy: 0.9951 - loss: 0.0141 - val_accuracy: 0.9246 - val_loss: 0.7417\n"
     ]
    }
   ],
   "source": [
    "#COnjunto !!!!!!!!!\n",
    "# 1. Carregando os dados\n",
    "data = data.dropna()\n",
    "X = data[\"X_Text_input\"]\n",
    "y = data[\"Y_CONJUNTO_output\"]  # Trabalhando apenas com Y_SISTEMA_output\n",
    "# 2. Pré-processando a saída\n",
    "# Convertendo as categorias da saída para valores numéricos\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# 3. Dividindo os dados\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4. Tokenização e padronização das sequências\n",
    "max_words = 10000  # Número máximo de palavras no vocabulário\n",
    "max_len = 100      # Tamanho máximo de sequência\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<UNK>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "# 5. Criando o modelo\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=max_words, output_dim=128),\n",
    "    LSTM(128, return_sequences=True),\n",
    "    GlobalMaxPool1D(),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')  # Classificação multiclasse\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 6. Treinando o modelo\n",
    "history = model.fit(\n",
    "    X_train_padded, y_train,\n",
    "    validation_data=(X_test_padded, y_test),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "#salvando movel\n",
    "#COnjunto !!!!!!!!!\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import save_model\n",
    "import pickle\n",
    "\n",
    "# Salvar o modelo treinado\n",
    "model.save(\"modelo_classificacao_y_CONJUNTO.h5\")\n",
    "\n",
    "# Salvar o tokenizer\n",
    "with open(\"tokenizer_CONJUNTO.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "# Salvar o LabelEncoder\n",
    "with open(\"label_encoder_CONJUNTO.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label_encoder, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 287ms/step\n",
      "Entrada: 17.02 - rod 1 esq escoamento de material + roda 1 ld friso fino 18mm e quebradiço - não carregar no porto - ATA\n",
      "Previsão: RODEIRO\n"
     ]
    }
   ],
   "source": [
    "#testando model\n",
    "#COnjunto !!!!!!!!!\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "# 4. Tokenização e padronização das sequências\n",
    "max_words = 10000  # Número máximo de palavras no vocabulário\n",
    "max_len = 100      # Tamanho máximo de sequência\n",
    "\n",
    "# Carregar o modelo\n",
    "modelo_carregado = load_model(\"modelo_classificacao_y_CONJUNTO.h5\")\n",
    "\n",
    "# Carregar o tokenizer\n",
    "with open(\"tokenizer_CONJUNTO.pkl\", \"rb\") as f:\n",
    "    tokenizer_carregado = pickle.load(f)\n",
    "\n",
    "# Carregar o LabelEncoder\n",
    "with open(\"label_encoder_CONJUNTO.pkl\", \"rb\") as f:\n",
    "    label_encoder_carregado = pickle.load(f)\n",
    "\n",
    "# Fazer previsão com o modelo carregado\n",
    "novo_texto = [\"17.02 - rod 1 esq escoamento de material + roda 1 ld friso fino 18mm e quebradiço - não carregar no porto - ATA\"]\n",
    "novo_texto_seq = tokenizer_carregado.texts_to_sequences(novo_texto)\n",
    "novo_texto_padded = pad_sequences(novo_texto_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "pred_proba = modelo_carregado.predict(novo_texto_padded)\n",
    "pred_classe = np.argmax(pred_proba, axis=1)\n",
    "pred_label = label_encoder_carregado.inverse_transform(pred_classe)\n",
    "\n",
    "print(f\"Entrada: {novo_texto[0]}\")\n",
    "print(f\"Previsão: {pred_label[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 60ms/step - accuracy: 0.2259 - loss: 3.5001 - val_accuracy: 0.7026 - val_loss: 1.3487\n",
      "Epoch 2/5\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 59ms/step - accuracy: 0.7287 - loss: 1.2900 - val_accuracy: 0.7895 - val_loss: 0.9215\n",
      "Epoch 3/5\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 57ms/step - accuracy: 0.8077 - loss: 0.8536 - val_accuracy: 0.8119 - val_loss: 0.7541\n",
      "Epoch 4/5\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 60ms/step - accuracy: 0.8412 - loss: 0.6562 - val_accuracy: 0.8248 - val_loss: 0.6925\n",
      "Epoch 5/5\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 56ms/step - accuracy: 0.8552 - loss: 0.5786 - val_accuracy: 0.8368 - val_loss: 0.6754\n"
     ]
    }
   ],
   "source": [
    "#Item <--------------------------------------------------------------------------------\n",
    "\n",
    "# 1. Carregando os dados\n",
    "data = data.dropna()\n",
    "X = data[\"X_Text_input\"]\n",
    "y = data[\"Y_ITEM_output\"]  # Trabalhando apenas com Y_SISTEMA_output\n",
    "# 2. Pré-processando a saída\n",
    "# Convertendo as categorias da saída para valores numéricos\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# 3. Dividindo os dados\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4. Tokenização e padronização das sequências\n",
    "max_words = 10000  # Número máximo de palavras no vocabulário\n",
    "max_len = 100      # Tamanho máximo de sequência\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<UNK>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "# 5. Criando o modelo\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=max_words, output_dim=128),\n",
    "    LSTM(128, return_sequences=True),\n",
    "    GlobalMaxPool1D(),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')  # Classificação multiclasse\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 6. Treinando o modelo\n",
    "history = model.fit(\n",
    "    X_train_padded, y_train,\n",
    "    validation_data=(X_test_padded, y_test),\n",
    "    epochs=5,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "#salvando movel\n",
    "#Item <--------------------------------------------------------------------------------\n",
    "from tensorflow.keras.models import save_model\n",
    "import pickle\n",
    "\n",
    "# Salvar o modelo treinado\n",
    "model.save(\"modelo_classificacao_y_Item.h5\")\n",
    "\n",
    "# Salvar o tokenizer\n",
    "with open(\"tokenizer_Item.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "# Salvar o LabelEncoder\n",
    "with open(\"label_encoder_Item.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label_encoder, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step\n",
      "Entrada: roda 3e detecÇÃo wcm médio 248kn\n",
      "Previsão: WCM\n"
     ]
    }
   ],
   "source": [
    "#testando model\n",
    "#Item <--------------------------------------------------------------------------------\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "\n",
    "# Carregar o modelo\n",
    "modelo_carregado = load_model(\"modelo_classificacao_y_Item.h5\")\n",
    "\n",
    "# Carregar o tokenizer\n",
    "with open(\"tokenizer_Item.pkl\", \"rb\") as f:\n",
    "    tokenizer_carregado = pickle.load(f)\n",
    "\n",
    "# Carregar o LabelEncoder\n",
    "with open(\"label_encoder_Item.pkl\", \"rb\") as f:\n",
    "    label_encoder_carregado = pickle.load(f)\n",
    "\n",
    "# Fazer previsão com o modelo carregado\n",
    "novo_texto = [\"roda 3e detecÇÃo wcm médio 248kn\"]\n",
    "novo_texto_seq = tokenizer_carregado.texts_to_sequences(novo_texto)\n",
    "novo_texto_padded = pad_sequences(novo_texto_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "pred_proba = modelo_carregado.predict(novo_texto_padded)\n",
    "pred_classe = np.argmax(pred_proba, axis=1)\n",
    "pred_label = label_encoder_carregado.inverse_transform(pred_classe)\n",
    "\n",
    "print(f\"Entrada: {novo_texto[0]}\")\n",
    "print(f\"Previsão: {pred_label[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 57ms/step - accuracy: 0.1819 - loss: 3.6387 - val_accuracy: 0.5995 - val_loss: 1.8619\n",
      "Epoch 2/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 56ms/step - accuracy: 0.6406 - loss: 1.6341 - val_accuracy: 0.6764 - val_loss: 1.3685\n",
      "Epoch 3/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - accuracy: 0.7183 - loss: 1.1609 - val_accuracy: 0.7270 - val_loss: 1.1986\n",
      "Epoch 4/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - accuracy: 0.7716 - loss: 0.9194 - val_accuracy: 0.7480 - val_loss: 1.0882\n",
      "Epoch 5/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 59ms/step - accuracy: 0.8056 - loss: 0.7588 - val_accuracy: 0.7675 - val_loss: 1.0430\n",
      "Epoch 6/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 56ms/step - accuracy: 0.8227 - loss: 0.6764 - val_accuracy: 0.7728 - val_loss: 1.0195\n",
      "Epoch 7/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 57ms/step - accuracy: 0.8542 - loss: 0.5458 - val_accuracy: 0.7838 - val_loss: 1.0287\n",
      "Epoch 8/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 57ms/step - accuracy: 0.8573 - loss: 0.5228 - val_accuracy: 0.7833 - val_loss: 1.0677\n",
      "Epoch 9/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - accuracy: 0.8664 - loss: 0.4656 - val_accuracy: 0.7885 - val_loss: 1.0727\n",
      "Epoch 10/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - accuracy: 0.8781 - loss: 0.4229 - val_accuracy: 0.7900 - val_loss: 1.1144\n",
      "Epoch 11/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - accuracy: 0.8893 - loss: 0.3798 - val_accuracy: 0.7866 - val_loss: 1.1837\n",
      "Epoch 12/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - accuracy: 0.8967 - loss: 0.3513 - val_accuracy: 0.7919 - val_loss: 1.1926\n",
      "Epoch 13/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - accuracy: 0.9034 - loss: 0.3192 - val_accuracy: 0.7862 - val_loss: 1.2040\n",
      "Epoch 14/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 57ms/step - accuracy: 0.9025 - loss: 0.3308 - val_accuracy: 0.7957 - val_loss: 1.2653\n",
      "Epoch 15/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 55ms/step - accuracy: 0.9185 - loss: 0.2893 - val_accuracy: 0.7900 - val_loss: 1.2566\n",
      "Epoch 16/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 56ms/step - accuracy: 0.9189 - loss: 0.2728 - val_accuracy: 0.7938 - val_loss: 1.3072\n",
      "Epoch 17/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 56ms/step - accuracy: 0.9237 - loss: 0.2474 - val_accuracy: 0.7957 - val_loss: 1.2854\n",
      "Epoch 18/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - accuracy: 0.9215 - loss: 0.2573 - val_accuracy: 0.7890 - val_loss: 1.3535\n",
      "Epoch 19/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - accuracy: 0.9231 - loss: 0.2455 - val_accuracy: 0.7967 - val_loss: 1.3334\n",
      "Epoch 20/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - accuracy: 0.9313 - loss: 0.2228 - val_accuracy: 0.8005 - val_loss: 1.3716\n",
      "Epoch 21/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 55ms/step - accuracy: 0.9335 - loss: 0.2115 - val_accuracy: 0.8062 - val_loss: 1.3477\n",
      "Epoch 22/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - accuracy: 0.9321 - loss: 0.2193 - val_accuracy: 0.8029 - val_loss: 1.3729\n",
      "Epoch 23/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 55ms/step - accuracy: 0.9350 - loss: 0.1944 - val_accuracy: 0.7981 - val_loss: 1.4263\n",
      "Epoch 24/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - accuracy: 0.9453 - loss: 0.1787 - val_accuracy: 0.8000 - val_loss: 1.4685\n",
      "Epoch 25/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - accuracy: 0.9491 - loss: 0.1578 - val_accuracy: 0.7981 - val_loss: 1.4480\n",
      "Epoch 26/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 57ms/step - accuracy: 0.9533 - loss: 0.1482 - val_accuracy: 0.7900 - val_loss: 1.5371\n",
      "Epoch 27/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - accuracy: 0.9464 - loss: 0.1615 - val_accuracy: 0.8010 - val_loss: 1.5592\n",
      "Epoch 28/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 56ms/step - accuracy: 0.9509 - loss: 0.1559 - val_accuracy: 0.7914 - val_loss: 1.6019\n",
      "Epoch 29/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - accuracy: 0.9543 - loss: 0.1451 - val_accuracy: 0.7919 - val_loss: 1.6681\n",
      "Epoch 30/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - accuracy: 0.9541 - loss: 0.1383 - val_accuracy: 0.7981 - val_loss: 1.6150\n",
      "Epoch 31/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 55ms/step - accuracy: 0.9565 - loss: 0.1344 - val_accuracy: 0.7952 - val_loss: 1.6746\n",
      "Epoch 32/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - accuracy: 0.9587 - loss: 0.1283 - val_accuracy: 0.7780 - val_loss: 1.7501\n",
      "Epoch 33/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - accuracy: 0.9509 - loss: 0.1527 - val_accuracy: 0.7885 - val_loss: 1.7389\n",
      "Epoch 34/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 56ms/step - accuracy: 0.9521 - loss: 0.1471 - val_accuracy: 0.7914 - val_loss: 1.6987\n",
      "Epoch 35/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 56ms/step - accuracy: 0.9613 - loss: 0.1146 - val_accuracy: 0.7909 - val_loss: 1.7208\n",
      "Epoch 36/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 57ms/step - accuracy: 0.9620 - loss: 0.1167 - val_accuracy: 0.7990 - val_loss: 1.7600\n",
      "Epoch 37/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - accuracy: 0.9645 - loss: 0.1169 - val_accuracy: 0.7981 - val_loss: 1.7304\n",
      "Epoch 38/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - accuracy: 0.9645 - loss: 0.1061 - val_accuracy: 0.7986 - val_loss: 1.7869\n",
      "Epoch 39/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 57ms/step - accuracy: 0.9632 - loss: 0.1071 - val_accuracy: 0.7947 - val_loss: 1.8303\n",
      "Epoch 40/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 55ms/step - accuracy: 0.9637 - loss: 0.1111 - val_accuracy: 0.7952 - val_loss: 1.7577\n",
      "Epoch 41/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - accuracy: 0.9672 - loss: 0.1048 - val_accuracy: 0.7952 - val_loss: 1.8409\n",
      "Epoch 42/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 55ms/step - accuracy: 0.9670 - loss: 0.0924 - val_accuracy: 0.7928 - val_loss: 1.8830\n",
      "Epoch 43/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - accuracy: 0.9683 - loss: 0.0939 - val_accuracy: 0.7981 - val_loss: 1.9444\n",
      "Epoch 44/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 55ms/step - accuracy: 0.9687 - loss: 0.0898 - val_accuracy: 0.7905 - val_loss: 1.9422\n",
      "Epoch 45/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 56ms/step - accuracy: 0.9656 - loss: 0.0906 - val_accuracy: 0.7924 - val_loss: 1.9645\n",
      "Epoch 46/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 57ms/step - accuracy: 0.9701 - loss: 0.0930 - val_accuracy: 0.7971 - val_loss: 1.9424\n",
      "Epoch 47/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 57ms/step - accuracy: 0.9675 - loss: 0.0913 - val_accuracy: 0.7971 - val_loss: 1.9901\n",
      "Epoch 48/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 56ms/step - accuracy: 0.9666 - loss: 0.0972 - val_accuracy: 0.7924 - val_loss: 1.9331\n",
      "Epoch 49/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 56ms/step - accuracy: 0.9722 - loss: 0.0846 - val_accuracy: 0.7924 - val_loss: 1.9243\n",
      "Epoch 50/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 55ms/step - accuracy: 0.9701 - loss: 0.0910 - val_accuracy: 0.7957 - val_loss: 1.9194\n"
     ]
    }
   ],
   "source": [
    "#Problema <--------------------------------------------------------------------------------\n",
    "\n",
    "# 1. Carregando os dados\n",
    "data = data.dropna()\n",
    "X = data[\"X_Text_input\"]\n",
    "y = data[\"Y_PROBLEMA_output\"]  # Trabalhando apenas com Y_SISTEMA_output\n",
    "# 2. Pré-processando a saída\n",
    "# Convertendo as categorias da saída para valores numéricos\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# 3. Dividindo os dados\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4. Tokenização e padronização das sequências\n",
    "max_words = 10000  # Número máximo de palavras no vocabulário\n",
    "max_len = 100      # Tamanho máximo de sequência\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<UNK>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "# 5. Criando o modelo\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=max_words, output_dim=128),\n",
    "    LSTM(128, return_sequences=True),\n",
    "    GlobalMaxPool1D(),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')  # Classificação multiclasse\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 6. Treinando o modelo\n",
    "history = model.fit(\n",
    "    X_train_padded, y_train,\n",
    "    validation_data=(X_test_padded, y_test),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "#salvando movel\n",
    "#Problema <--------------------------------------------------------------------------------\n",
    "from tensorflow.keras.models import save_model\n",
    "import pickle\n",
    "\n",
    "# Salvar o modelo treinado\n",
    "model.save(\"modelo_classificacao_y_Problema.h5\")\n",
    "\n",
    "# Salvar o tokenizer\n",
    "with open(\"tokenizer_PROBLEMA.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "# Salvar o LabelEncoder\n",
    "with open(\"label_encoder_PROBLEMA.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label_encoder, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234ms/step\n",
      "Entrada: 11/01 |CA-LD|240 mm|||LONG BAT DIANTEIRO TRINC\n",
      "Previsão: TRINCADA\n"
     ]
    }
   ],
   "source": [
    "#testando model\n",
    "#Problema <--------------------------------------------------------------------------------\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "\n",
    "# Carregar o modelo\n",
    "modelo_carregado = load_model(\"modelo_classificacao_y_Problema.h5\")\n",
    "\n",
    "# Carregar o tokenizer\n",
    "with open(\"tokenizer_PROBLEMA.pkl\", \"rb\") as f:\n",
    "    tokenizer_carregado = pickle.load(f)\n",
    "\n",
    "# Carregar o LabelEncoder\n",
    "with open(\"label_encoder_PROBLEMA.pkl\", \"rb\") as f:\n",
    "    label_encoder_carregado = pickle.load(f)\n",
    "\n",
    "# Fazer previsão com o modelo carregado\n",
    "novo_texto = [\"11/01 |CA-LD|240 mm|||LONG BAT DIANTEIRO TRINC\"]\n",
    "novo_texto_seq = tokenizer_carregado.texts_to_sequences(novo_texto)\n",
    "novo_texto_padded = pad_sequences(novo_texto_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "pred_proba = modelo_carregado.predict(novo_texto_padded)\n",
    "pred_classe = np.argmax(pred_proba, axis=1)\n",
    "pred_label = label_encoder_carregado.inverse_transform(pred_classe)\n",
    "\n",
    "print(f\"Entrada: {novo_texto[0]}\")\n",
    "print(f\"Previsão: {pred_label[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 119ms/step - accuracy: 0.2380 - loss: 3.9389 - val_accuracy: 0.7442 - val_loss: 1.3728\n",
      "Epoch 2/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 122ms/step - accuracy: 0.7604 - loss: 1.1654 - val_accuracy: 0.7876 - val_loss: 1.0328\n",
      "Epoch 3/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 119ms/step - accuracy: 0.7992 - loss: 0.8261 - val_accuracy: 0.8119 - val_loss: 0.9585\n",
      "Epoch 4/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 116ms/step - accuracy: 0.8242 - loss: 0.6641 - val_accuracy: 0.8162 - val_loss: 0.9109\n",
      "Epoch 5/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 123ms/step - accuracy: 0.8349 - loss: 0.5481 - val_accuracy: 0.8334 - val_loss: 0.9066\n",
      "Epoch 6/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 130ms/step - accuracy: 0.8525 - loss: 0.4811 - val_accuracy: 0.8315 - val_loss: 0.9183\n",
      "Epoch 7/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 137ms/step - accuracy: 0.8590 - loss: 0.4342 - val_accuracy: 0.8406 - val_loss: 0.9347\n",
      "Epoch 8/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 137ms/step - accuracy: 0.8591 - loss: 0.4054 - val_accuracy: 0.8406 - val_loss: 0.9467\n",
      "Epoch 9/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 136ms/step - accuracy: 0.8569 - loss: 0.3993 - val_accuracy: 0.8411 - val_loss: 0.9560\n",
      "Epoch 10/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 127ms/step - accuracy: 0.8610 - loss: 0.3820 - val_accuracy: 0.8368 - val_loss: 0.9634\n",
      "Epoch 11/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 129ms/step - accuracy: 0.8556 - loss: 0.3803 - val_accuracy: 0.8387 - val_loss: 0.9806\n",
      "Epoch 12/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 128ms/step - accuracy: 0.8660 - loss: 0.3629 - val_accuracy: 0.8372 - val_loss: 0.9997\n",
      "Epoch 13/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 130ms/step - accuracy: 0.8674 - loss: 0.3588 - val_accuracy: 0.8372 - val_loss: 0.9999\n",
      "Epoch 14/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 133ms/step - accuracy: 0.8755 - loss: 0.3413 - val_accuracy: 0.8430 - val_loss: 0.9883\n",
      "Epoch 15/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 130ms/step - accuracy: 0.8639 - loss: 0.3435 - val_accuracy: 0.8473 - val_loss: 0.9936\n",
      "Epoch 16/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 130ms/step - accuracy: 0.8770 - loss: 0.3380 - val_accuracy: 0.8387 - val_loss: 0.9880\n",
      "Epoch 17/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 130ms/step - accuracy: 0.8727 - loss: 0.3315 - val_accuracy: 0.8411 - val_loss: 1.0052\n",
      "Epoch 18/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 136ms/step - accuracy: 0.8776 - loss: 0.3228 - val_accuracy: 0.8396 - val_loss: 0.9846\n",
      "Epoch 19/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 133ms/step - accuracy: 0.8724 - loss: 0.3405 - val_accuracy: 0.8449 - val_loss: 1.0171\n",
      "Epoch 20/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 130ms/step - accuracy: 0.8778 - loss: 0.3166 - val_accuracy: 0.8453 - val_loss: 1.0073\n",
      "Epoch 21/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 131ms/step - accuracy: 0.8819 - loss: 0.3167 - val_accuracy: 0.8415 - val_loss: 0.9893\n",
      "Epoch 22/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 128ms/step - accuracy: 0.8783 - loss: 0.3138 - val_accuracy: 0.8430 - val_loss: 1.0030\n",
      "Epoch 23/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 131ms/step - accuracy: 0.8727 - loss: 0.3258 - val_accuracy: 0.8444 - val_loss: 1.0074\n",
      "Epoch 24/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 127ms/step - accuracy: 0.8768 - loss: 0.3135 - val_accuracy: 0.8430 - val_loss: 0.9820\n",
      "Epoch 25/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 130ms/step - accuracy: 0.8750 - loss: 0.3116 - val_accuracy: 0.8458 - val_loss: 1.0168\n",
      "Epoch 26/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 129ms/step - accuracy: 0.8742 - loss: 0.3205 - val_accuracy: 0.8425 - val_loss: 1.0055\n",
      "Epoch 27/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 130ms/step - accuracy: 0.8831 - loss: 0.3036 - val_accuracy: 0.8420 - val_loss: 0.9932\n",
      "Epoch 28/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 134ms/step - accuracy: 0.8733 - loss: 0.3113 - val_accuracy: 0.8482 - val_loss: 1.0333\n",
      "Epoch 29/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 134ms/step - accuracy: 0.8789 - loss: 0.3064 - val_accuracy: 0.8463 - val_loss: 1.0193\n",
      "Epoch 30/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 132ms/step - accuracy: 0.8807 - loss: 0.3187 - val_accuracy: 0.8473 - val_loss: 0.9721\n",
      "Epoch 31/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 135ms/step - accuracy: 0.8678 - loss: 0.3153 - val_accuracy: 0.8453 - val_loss: 1.0138\n",
      "Epoch 32/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 131ms/step - accuracy: 0.8772 - loss: 0.3088 - val_accuracy: 0.8401 - val_loss: 0.9881\n",
      "Epoch 33/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 131ms/step - accuracy: 0.8778 - loss: 0.3123 - val_accuracy: 0.8420 - val_loss: 1.0132\n",
      "Epoch 34/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 131ms/step - accuracy: 0.8805 - loss: 0.3075 - val_accuracy: 0.8453 - val_loss: 1.0188\n",
      "Epoch 35/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 130ms/step - accuracy: 0.8762 - loss: 0.3175 - val_accuracy: 0.8401 - val_loss: 1.0504\n",
      "Epoch 36/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 135ms/step - accuracy: 0.8802 - loss: 0.3061 - val_accuracy: 0.8430 - val_loss: 1.0250\n",
      "Epoch 37/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 128ms/step - accuracy: 0.8753 - loss: 0.3122 - val_accuracy: 0.8458 - val_loss: 1.0113\n",
      "Epoch 38/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 131ms/step - accuracy: 0.8883 - loss: 0.2887 - val_accuracy: 0.8334 - val_loss: 1.0249\n",
      "Epoch 39/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 130ms/step - accuracy: 0.8750 - loss: 0.3046 - val_accuracy: 0.8434 - val_loss: 0.9890\n",
      "Epoch 40/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 130ms/step - accuracy: 0.8810 - loss: 0.2978 - val_accuracy: 0.8468 - val_loss: 1.0357\n",
      "Epoch 41/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 128ms/step - accuracy: 0.8817 - loss: 0.2962 - val_accuracy: 0.8458 - val_loss: 1.0241\n",
      "Epoch 42/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 131ms/step - accuracy: 0.8846 - loss: 0.2933 - val_accuracy: 0.8468 - val_loss: 1.0557\n",
      "Epoch 43/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 132ms/step - accuracy: 0.8824 - loss: 0.2918 - val_accuracy: 0.8439 - val_loss: 1.0092\n",
      "Epoch 44/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 125ms/step - accuracy: 0.8796 - loss: 0.3033 - val_accuracy: 0.8434 - val_loss: 1.0283\n",
      "Epoch 45/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 122ms/step - accuracy: 0.8869 - loss: 0.2766 - val_accuracy: 0.8482 - val_loss: 1.0066\n",
      "Epoch 46/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 121ms/step - accuracy: 0.8799 - loss: 0.2998 - val_accuracy: 0.8463 - val_loss: 1.0263\n",
      "Epoch 47/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 124ms/step - accuracy: 0.8802 - loss: 0.3043 - val_accuracy: 0.8501 - val_loss: 1.0324\n",
      "Epoch 48/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 121ms/step - accuracy: 0.8772 - loss: 0.3013 - val_accuracy: 0.8439 - val_loss: 1.0089\n",
      "Epoch 49/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 126ms/step - accuracy: 0.8744 - loss: 0.3025 - val_accuracy: 0.8468 - val_loss: 1.0342\n",
      "Epoch 50/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 124ms/step - accuracy: 0.8835 - loss: 0.2919 - val_accuracy: 0.8468 - val_loss: 1.0236\n",
      "Epoch 51/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 123ms/step - accuracy: 0.8756 - loss: 0.3129 - val_accuracy: 0.8477 - val_loss: 1.0428\n",
      "Epoch 52/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 124ms/step - accuracy: 0.8758 - loss: 0.2975 - val_accuracy: 0.8477 - val_loss: 1.0208\n",
      "Epoch 53/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 126ms/step - accuracy: 0.8819 - loss: 0.2955 - val_accuracy: 0.8477 - val_loss: 1.0136\n",
      "Epoch 54/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 128ms/step - accuracy: 0.8831 - loss: 0.2947 - val_accuracy: 0.8468 - val_loss: 1.0422\n",
      "Epoch 55/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 131ms/step - accuracy: 0.8827 - loss: 0.2899 - val_accuracy: 0.8449 - val_loss: 1.0737\n",
      "Epoch 56/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 131ms/step - accuracy: 0.8813 - loss: 0.2831 - val_accuracy: 0.8453 - val_loss: 1.0065\n",
      "Epoch 57/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 129ms/step - accuracy: 0.8772 - loss: 0.3041 - val_accuracy: 0.8487 - val_loss: 1.0244\n",
      "Epoch 58/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 125ms/step - accuracy: 0.8842 - loss: 0.2927 - val_accuracy: 0.8492 - val_loss: 1.0442\n",
      "Epoch 59/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 128ms/step - accuracy: 0.8801 - loss: 0.3022 - val_accuracy: 0.8449 - val_loss: 1.0395\n",
      "Epoch 60/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 125ms/step - accuracy: 0.8784 - loss: 0.2960 - val_accuracy: 0.8387 - val_loss: 1.0444\n",
      "Epoch 61/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 129ms/step - accuracy: 0.8839 - loss: 0.2872 - val_accuracy: 0.8439 - val_loss: 1.1326\n",
      "Epoch 62/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 134ms/step - accuracy: 0.8837 - loss: 0.2892 - val_accuracy: 0.8463 - val_loss: 1.0578\n",
      "Epoch 63/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 127ms/step - accuracy: 0.8747 - loss: 0.2978 - val_accuracy: 0.8468 - val_loss: 1.0904\n",
      "Epoch 64/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 123ms/step - accuracy: 0.8790 - loss: 0.2921 - val_accuracy: 0.8444 - val_loss: 1.0777\n",
      "Epoch 65/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 120ms/step - accuracy: 0.8844 - loss: 0.2963 - val_accuracy: 0.8473 - val_loss: 1.1083\n",
      "Epoch 66/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 121ms/step - accuracy: 0.8745 - loss: 0.2965 - val_accuracy: 0.8473 - val_loss: 1.0681\n",
      "Epoch 67/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 123ms/step - accuracy: 0.8858 - loss: 0.2817 - val_accuracy: 0.8444 - val_loss: 1.0855\n",
      "Epoch 68/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 125ms/step - accuracy: 0.8759 - loss: 0.2981 - val_accuracy: 0.8434 - val_loss: 1.1371\n",
      "Epoch 69/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 123ms/step - accuracy: 0.8794 - loss: 0.2965 - val_accuracy: 0.8458 - val_loss: 1.0999\n",
      "Epoch 70/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 121ms/step - accuracy: 0.8775 - loss: 0.2867 - val_accuracy: 0.8477 - val_loss: 1.1055\n",
      "Epoch 71/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 125ms/step - accuracy: 0.8816 - loss: 0.2908 - val_accuracy: 0.8415 - val_loss: 1.1095\n",
      "Epoch 72/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 125ms/step - accuracy: 0.8849 - loss: 0.2800 - val_accuracy: 0.8453 - val_loss: 1.0687\n",
      "Epoch 73/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 122ms/step - accuracy: 0.8807 - loss: 0.2997 - val_accuracy: 0.8468 - val_loss: 1.1005\n",
      "Epoch 74/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 127ms/step - accuracy: 0.8885 - loss: 0.2837 - val_accuracy: 0.8458 - val_loss: 1.1117\n",
      "Epoch 75/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 121ms/step - accuracy: 0.8785 - loss: 0.2904 - val_accuracy: 0.8425 - val_loss: 1.0952\n",
      "Epoch 76/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 122ms/step - accuracy: 0.8819 - loss: 0.2847 - val_accuracy: 0.8453 - val_loss: 1.1284\n",
      "Epoch 77/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 126ms/step - accuracy: 0.8788 - loss: 0.2975 - val_accuracy: 0.8468 - val_loss: 1.1324\n",
      "Epoch 78/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 130ms/step - accuracy: 0.8816 - loss: 0.2899 - val_accuracy: 0.8449 - val_loss: 1.1240\n",
      "Epoch 79/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 126ms/step - accuracy: 0.8875 - loss: 0.2772 - val_accuracy: 0.8401 - val_loss: 1.1047\n",
      "Epoch 80/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 125ms/step - accuracy: 0.8867 - loss: 0.2800 - val_accuracy: 0.8473 - val_loss: 1.1164\n",
      "Epoch 81/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 127ms/step - accuracy: 0.8796 - loss: 0.2892 - val_accuracy: 0.8477 - val_loss: 1.1023\n",
      "Epoch 82/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 127ms/step - accuracy: 0.8875 - loss: 0.2720 - val_accuracy: 0.8468 - val_loss: 1.0977\n",
      "Epoch 83/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 125ms/step - accuracy: 0.8847 - loss: 0.2870 - val_accuracy: 0.8430 - val_loss: 1.1487\n",
      "Epoch 84/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 127ms/step - accuracy: 0.8787 - loss: 0.2983 - val_accuracy: 0.8439 - val_loss: 1.1131\n",
      "Epoch 85/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 130ms/step - accuracy: 0.8821 - loss: 0.2847 - val_accuracy: 0.8396 - val_loss: 1.1387\n",
      "Epoch 86/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 130ms/step - accuracy: 0.8821 - loss: 0.2866 - val_accuracy: 0.8463 - val_loss: 1.1463\n",
      "Epoch 87/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 127ms/step - accuracy: 0.8853 - loss: 0.2872 - val_accuracy: 0.8444 - val_loss: 1.1582\n",
      "Epoch 88/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 126ms/step - accuracy: 0.8821 - loss: 0.2944 - val_accuracy: 0.8463 - val_loss: 1.1273\n",
      "Epoch 89/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 125ms/step - accuracy: 0.8749 - loss: 0.2952 - val_accuracy: 0.8463 - val_loss: 1.1521\n",
      "Epoch 90/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 125ms/step - accuracy: 0.8847 - loss: 0.2863 - val_accuracy: 0.8449 - val_loss: 1.0677\n",
      "Epoch 91/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 125ms/step - accuracy: 0.8828 - loss: 0.2826 - val_accuracy: 0.8458 - val_loss: 1.1314\n",
      "Epoch 92/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 130ms/step - accuracy: 0.8871 - loss: 0.2846 - val_accuracy: 0.8425 - val_loss: 1.1324\n",
      "Epoch 93/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 126ms/step - accuracy: 0.8805 - loss: 0.2841 - val_accuracy: 0.8492 - val_loss: 1.0866\n",
      "Epoch 94/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 132ms/step - accuracy: 0.8923 - loss: 0.2747 - val_accuracy: 0.8434 - val_loss: 1.1145\n",
      "Epoch 95/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 128ms/step - accuracy: 0.8797 - loss: 0.2925 - val_accuracy: 0.8492 - val_loss: 1.1665\n",
      "Epoch 96/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 125ms/step - accuracy: 0.8828 - loss: 0.2904 - val_accuracy: 0.8468 - val_loss: 1.1161\n",
      "Epoch 97/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 125ms/step - accuracy: 0.8743 - loss: 0.2989 - val_accuracy: 0.8434 - val_loss: 1.1707\n",
      "Epoch 98/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 125ms/step - accuracy: 0.8836 - loss: 0.2846 - val_accuracy: 0.8444 - val_loss: 1.1159\n",
      "Epoch 99/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 128ms/step - accuracy: 0.8799 - loss: 0.2921 - val_accuracy: 0.8449 - val_loss: 1.0847\n",
      "Epoch 100/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 127ms/step - accuracy: 0.8878 - loss: 0.2724 - val_accuracy: 0.8482 - val_loss: 1.0983\n"
     ]
    }
   ],
   "source": [
    "# modelo final \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Concatenate, GlobalMaxPool1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "\n",
    "# 1. Carregando os dados\n",
    "data = data.dropna()\n",
    "X = data[[\"Y_SISTEMA_output\", \"Y_CONJUNTO_output\", \"Y_ITEM_output\", \"Y_PROBLEMA_output\"]]\n",
    "y = data[\"Y_OCORRÊNCIA_output\"]\n",
    "\n",
    "# 2. Codificando os rótulos da saída\n",
    "label_encoder_y = LabelEncoder()\n",
    "y_encoded = label_encoder_y.fit_transform(y)\n",
    "\n",
    "# 3. Dividindo os dados\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4. Tokenização e padronização das entradas\n",
    "max_words = 10000  # Número máximo de palavras no vocabulário\n",
    "max_len = 100       # Tamanho máximo de sequência\n",
    "\n",
    "tokenizers = {}\n",
    "X_train_padded = {}\n",
    "X_test_padded = {}\n",
    "\n",
    "# Criar um tokenizer para cada entrada categórica\n",
    "for column in X.columns:\n",
    "    tokenizer = Tokenizer(num_words=max_words, oov_token=\"<UNK>\")\n",
    "    tokenizer.fit_on_texts(X_train[column])\n",
    "    tokenizers[column] = tokenizer\n",
    "    # Tokenizar e padronizar as entradas\n",
    "    X_train_padded[column] = pad_sequences(tokenizer.texts_to_sequences(X_train[column]), maxlen=max_len, padding='post')\n",
    "    X_test_padded[column] = pad_sequences(tokenizer.texts_to_sequences(X_test[column]), maxlen=max_len, padding='post')\n",
    "\n",
    "# 5. Criando o modelo\n",
    "input_layers = []\n",
    "embedding_layers = []\n",
    "\n",
    "for column in X.columns:\n",
    "    input_layer = Input(shape=(max_len,), name=f\"input_{column}\")\n",
    "    embedding_layer = Embedding(input_dim=max_words, output_dim=128)(input_layer)\n",
    "    lstm_layer = LSTM(64, return_sequences=True)(embedding_layer)\n",
    "    pooled_layer = GlobalMaxPool1D()(lstm_layer)\n",
    "    input_layers.append(input_layer)\n",
    "    embedding_layers.append(pooled_layer)\n",
    "\n",
    "# Concatenar todas as saídas de embedding\n",
    "concatenated = Concatenate()(embedding_layers)\n",
    "dense1 = Dense(128, activation='relu')(concatenated)\n",
    "dense2 = Dense(64, activation='relu')(dense1)\n",
    "output_layer = Dense(len(label_encoder_y.classes_), activation='softmax', name=\"output_y_ocorrencia\")(dense2)\n",
    "\n",
    "# Criar o modelo final\n",
    "model = Model(inputs=input_layers, outputs=output_layer)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 6. Treinando o modelo\n",
    "history = model.fit(\n",
    "    [X_train_padded[col] for col in X.columns], y_train,\n",
    "    validation_data=([X_test_padded[col] for col in X.columns], y_test),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.8416 - loss: 1.2560\n",
      "Loss: 1.0983006954193115, Accuracy: 0.8482100367546082\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 628ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrada: {'Y_SISTEMA_output': ['TRUQUE'], 'Y_CONJUNTO_output': ['TRAVESSA CENTRAL'], 'Y_ITEM_output': ['MOLA CUNHA'], 'Y_PROBLEMA_output': ['QUEBRADO']}\n",
      "Previsão: CUNHA AVARIADA\n"
     ]
    }
   ],
   "source": [
    "# modelo final \n",
    "# 7. Avaliando o modelo\n",
    "loss, accuracy = model.evaluate([X_test_padded[col] for col in X.columns], y_test)\n",
    "print(f\"Loss: {loss}, Accuracy: {accuracy}\")\n",
    "\n",
    "# 8. Fazendo previsões\n",
    "novo_exemplo = {\n",
    "    \"Y_SISTEMA_output\": [\"TRUQUE\"],\n",
    "    \"Y_CONJUNTO_output\": [\"TRAVESSA CENTRAL\"],\n",
    "    \"Y_ITEM_output\": [\"MOLA CUNHA\"],\n",
    "    \"Y_PROBLEMA_output\": [\"QUEBRADO\"]\n",
    "}\n",
    "\n",
    "# Processar as novas entradas\n",
    "novo_exemplo_padded = [\n",
    "    pad_sequences(tokenizers[col].texts_to_sequences(novo_exemplo[col]), maxlen=max_len, padding='post')\n",
    "    for col in novo_exemplo.keys()\n",
    "]\n",
    "\n",
    "# Previsão\n",
    "pred_proba = model.predict(novo_exemplo_padded)\n",
    "pred_class = np.argmax(pred_proba, axis=1)\n",
    "pred_label = label_encoder_y.inverse_transform(pred_class)\n",
    "\n",
    "print(f\"Entrada: {novo_exemplo}\")\n",
    "print(f\"Previsão: {pred_label[0]}\")\n",
    "\n",
    "# 9. Salvando o modelo e os tokenizers\n",
    "model.save(\"modelo_multientrada_y_OCORRÊNCIA.h5\")\n",
    "\n",
    "with open(\"tokenizers_OCORRÊNCIA.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizers, f)\n",
    "\n",
    "with open(\"label_encoder_y_OCORRÊNCIA.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label_encoder_y, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 582ms/step\n",
      "Entrada: {'Y_SISTEMA_output': ['CCT'], 'Y_CONJUNTO_output': ['CCT'], 'Y_ITEM_output': ['ROTOR'], 'Y_PROBLEMA_output': ['QUEBRADO']}\n",
      "Previsão (classe): ROTOR QUEBRADO\n"
     ]
    }
   ],
   "source": [
    "# modelo final \n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "\n",
    "# 1. Carregar o modelo salvo\n",
    "model = load_model(\"modelo_multientrada_y_OCORRÊNCIA.h5\")\n",
    "\n",
    "# 2. Carregar os preprocessadores salvos\n",
    "with open(\"tokenizers_OCORRÊNCIA.pkl\", \"rb\") as f:\n",
    "    tokenizers = pickle.load(f)\n",
    "\n",
    "with open(\"label_encoder_y_OCORRÊNCIA.pkl\", \"rb\") as f:\n",
    "    label_encoder_y = pickle.load(f)\n",
    "\n",
    "# 3. Definir uma nova entrada para teste\n",
    "novo_exemplo = {\n",
    "    \"Y_SISTEMA_output\": [\"CCT\"],\n",
    "    \"Y_CONJUNTO_output\": [\"CCT\"],\n",
    "    \"Y_ITEM_output\": [\"ROTOR\"],\n",
    "    \"Y_PROBLEMA_output\": [\"QUEBRADO\"]\n",
    "}\n",
    "\n",
    "# 4. Preprocessar as novas entradas\n",
    "max_len = 100  # Deve ser o mesmo usado ao treinar o modelo\n",
    "novo_exemplo_padded = [\n",
    "    pad_sequences(tokenizers[col].texts_to_sequences(novo_exemplo[col]), maxlen=max_len, padding='post')\n",
    "    for col in novo_exemplo.keys()\n",
    "]\n",
    "\n",
    "# 5. Fazer previsões\n",
    "pred_proba = model.predict(novo_exemplo_padded)\n",
    "pred_class = np.argmax(pred_proba, axis=1)\n",
    "pred_label = label_encoder_y.inverse_transform(pred_class)\n",
    "\n",
    "# 6. Exibir o resultado\n",
    "print(f\"Entrada: {novo_exemplo}\")\n",
    "print(f\"Previsão (classe): {pred_label[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
